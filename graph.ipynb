{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "# get_word_length.invoke(\"abc\")\n",
    "\n",
    "tools = [get_word_length, TavilySearchResults(max_results=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.openai_agent import create_agent\n",
    "\n",
    "model = create_agent(tools=tools, model_only=True, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation, ToolExecutor\n",
    "from langchain_core.messages import ToolMessage\n",
    "import json\n",
    "from modules.create_graph import create_graph, stream_app\n",
    "\n",
    "\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "tool_indicator = \"tool_calls\" #\"function_call\"\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if tool_indicator not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_agent(state):\n",
    "    messages = state['messages']\n",
    "    response = agent.invoke(\n",
    "        {\"input\": messages[0], \"chat_history\": chat.history})\n",
    "    return {\"messages\": [response['output']]}\n",
    "\n",
    "# Define the function to execute tools\n",
    "def call_tool(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    tool = last_message.additional_kwargs[tool_indicator][0]\n",
    "    action = ToolInvocation(\n",
    "        tool=tool['function'][\"name\"],\n",
    "        tool_input=json.loads(tool['function']['arguments'])\n",
    "    )\n",
    "\n",
    "    response = tool_executor.invoke(action)\n",
    "    function_message = ToolMessage(content=str(response), tool_call_id=tool['id'])\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [{\"name\": \"agent\", \"node\": call_model}, {\n",
    "    \"name\": \"action\", \"node\": call_tool, \"condition\": should_continue}]\n",
    "\n",
    "app = create_graph(nodes)\n",
    "\n",
    "user_input = \"what is the weather in sf\"\n",
    "stream_app(app, user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
